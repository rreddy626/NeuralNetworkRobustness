{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Neural Network Creation and Training:\n",
        "\n",
        "The following code creates a Neural Network from scratch, and trains the Neural Network based on the MNIST dataset"
      ],
      "metadata": {
        "id": "2amu9WKfX3RH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "o2lhZqdjXGio"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from helpers import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "gT2nHFLWXMBy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Structure adopted partially from the PyTorch Documentation page, basic Neural Network construction:\n",
        "\n",
        "# Note: The input images from MNIST will be in 28x28 dimensions, so linear input should be in 784:\n",
        "\n",
        "# Create the architecture for a very basic Neural Network:\n",
        "class MNISTNeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "      super().__init__()\n",
        "      self.flatten = nn.Flatten()\n",
        "      self.linear_relu_stack = nn.Sequential(\n",
        "          nn.Linear(input_size, 256),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(256, 256),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(256, output_size),\n",
        "      )\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.flatten(x)\n",
        "      logits = self.linear_relu_stack(x)\n",
        "      return logits"
      ],
      "metadata": {
        "id": "uci5nrcPX0IL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the MNIST Data:\n",
        "\n",
        "In order to load the MNIST Data into the notebook, we will need to make use of datasets and DataLoader modules as provided in PyTorch:"
      ],
      "metadata": {
        "id": "SgBy5kHwahIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset = datasets.MNIST(root='data/', train=True, download=True, transform=transforms.ToTensor())\n",
        "training_loader = DataLoader(training_dataset, batch_size=64)\n",
        "\n",
        "test_dataset = datasets.MNIST(root='data/', train=False, download=True, transform=transforms.ToTensor())\n",
        "test_loader = DataLoader(test_dataset, batch_size=64)"
      ],
      "metadata": {
        "id": "sIR-SFAVYoew",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe1f089e-79ad-4158-9673-fbc99b2ff49e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 17.9MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 481kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 3.87MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 6.61MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Network:\n",
        "model = MNISTNeuralNetwork(input_size = 784, output_size=10).to(device)\n",
        "\n",
        "# Declare Loss Function and Optimizer:\n",
        "loss_fcn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "optimizer.zero_grad()"
      ],
      "metadata": {
        "id": "dlURr2GFadP_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the Actual Network:\n",
        "\n",
        "For this project we will train the model on CLEAN data, but will test the neural network on noisy data. For this reason, we will need to alter the test dataset images later on in the notebook. However, for now - we will train the neural network that we have created over 50 epochs."
      ],
      "metadata": {
        "id": "aE4ffdMscq_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 50\n",
        "past_training_loss = np.inf\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "  training_loss = 0\n",
        "\n",
        "  for idx, (data, label) in enumerate(training_loader):\n",
        "    data = data.to(device)\n",
        "    labels = label.to(device)\n",
        "\n",
        "    data_flat = data.reshape(-1, 784)\n",
        "\n",
        "    # Make the prediction based on the NN model:\n",
        "    scores = model(data_flat)\n",
        "\n",
        "    # Compute the loss value based on the pre-defined loss function:\n",
        "    loss_val = loss_fcn(scores, labels)\n",
        "\n",
        "    # Backpropagation, allow for:\n",
        "    optimizer.zero_grad()\n",
        "    loss_val.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    training_loss += loss_val.item() * data.size(0)\n",
        "\n",
        "  print(f\"Epoch Number: {epoch}, Training Loss: {training_loss}\")\n",
        "  epoch += 1"
      ],
      "metadata": {
        "id": "2KJZ9hKxc54a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "816ee1c1-b68f-47ed-80c2-d094a765da5d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch Number: 0, Training Loss: 101300.79993057251\n",
            "Epoch Number: 1, Training Loss: 32304.3777384758\n",
            "Epoch Number: 2, Training Loss: 23235.27440929413\n",
            "Epoch Number: 3, Training Loss: 20239.67621064186\n",
            "Epoch Number: 4, Training Loss: 18430.0731613636\n",
            "Epoch Number: 5, Training Loss: 17063.020985126495\n",
            "Epoch Number: 6, Training Loss: 15927.504470586777\n",
            "Epoch Number: 7, Training Loss: 14932.963267803192\n",
            "Epoch Number: 8, Training Loss: 14041.836070775986\n",
            "Epoch Number: 9, Training Loss: 13231.25315272808\n",
            "Epoch Number: 10, Training Loss: 12489.769403636456\n",
            "Epoch Number: 11, Training Loss: 11809.711961269379\n",
            "Epoch Number: 12, Training Loss: 11185.74990093708\n",
            "Epoch Number: 13, Training Loss: 10611.091792285442\n",
            "Epoch Number: 14, Training Loss: 10081.262268006802\n",
            "Epoch Number: 15, Training Loss: 9593.343569934368\n",
            "Epoch Number: 16, Training Loss: 9143.080515027046\n",
            "Epoch Number: 17, Training Loss: 8724.285730421543\n",
            "Epoch Number: 18, Training Loss: 8333.70962792635\n",
            "Epoch Number: 19, Training Loss: 7970.06995049119\n",
            "Epoch Number: 20, Training Loss: 7630.434773057699\n",
            "Epoch Number: 21, Training Loss: 7313.389996051788\n",
            "Epoch Number: 22, Training Loss: 7016.388801574707\n",
            "Epoch Number: 23, Training Loss: 6737.760887473822\n",
            "Epoch Number: 24, Training Loss: 6476.049970507622\n",
            "Epoch Number: 25, Training Loss: 6228.782708376646\n",
            "Epoch Number: 26, Training Loss: 5995.061360090971\n",
            "Epoch Number: 27, Training Loss: 5773.248308077455\n",
            "Epoch Number: 28, Training Loss: 5564.393147811294\n",
            "Epoch Number: 29, Training Loss: 5366.086348623037\n",
            "Epoch Number: 30, Training Loss: 5178.5602958500385\n",
            "Epoch Number: 31, Training Loss: 4999.40288990736\n",
            "Epoch Number: 32, Training Loss: 4830.149748787284\n",
            "Epoch Number: 33, Training Loss: 4668.484099462628\n",
            "Epoch Number: 34, Training Loss: 4514.980241924524\n",
            "Epoch Number: 35, Training Loss: 4368.359949380159\n",
            "Epoch Number: 36, Training Loss: 4227.982745170593\n",
            "Epoch Number: 37, Training Loss: 4094.5129281431437\n",
            "Epoch Number: 38, Training Loss: 3966.6817138046026\n",
            "Epoch Number: 39, Training Loss: 3843.954932205379\n",
            "Epoch Number: 40, Training Loss: 3726.155565805733\n",
            "Epoch Number: 41, Training Loss: 3613.966901421547\n",
            "Epoch Number: 42, Training Loss: 3505.5221660509706\n",
            "Epoch Number: 43, Training Loss: 3401.895012423396\n",
            "Epoch Number: 44, Training Loss: 3301.8397837132215\n",
            "Epoch Number: 45, Training Loss: 3205.8565754815936\n",
            "Epoch Number: 46, Training Loss: 3113.562197238207\n",
            "Epoch Number: 47, Training Loss: 3024.8257617130876\n",
            "Epoch Number: 48, Training Loss: 2938.7617068886757\n",
            "Epoch Number: 49, Training Loss: 2856.072059303522\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing Accuracy and Loss:\n",
        "\n",
        "Now we will test the model on the testing data, and assess the loss values and accuracy:"
      ],
      "metadata": {
        "id": "n9bOetPWbvpX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_test_loss(model, test_loader):\n",
        "\n",
        "  \"\"\"Determines the test dataset loss, given a model\n",
        "\n",
        "  Parameters:\n",
        "  - model (MNISTNeuralNetwork) - The Machine Learning model used on the test dataset\n",
        "  - test_loader (torch.utils.data.dataloader.DataLoader) - The dataloader that is used to load the test data into the model\n",
        "\n",
        "  Returns:\n",
        "  - None (prints accuracy and test_loss metrics)\"\"\"\n",
        "\n",
        "  total_correct = 0\n",
        "  total_counter = 0\n",
        "\n",
        "  test_loss = 0\n",
        "\n",
        "  for data, labels in test_loader:\n",
        "\n",
        "    data = data.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    test_data_flat = data.reshape(-1, 784)\n",
        "\n",
        "    predictions = model(test_data_flat)\n",
        "\n",
        "    for pred, label in zip(predictions, labels):\n",
        "      if torch.argmax(pred).item() == label.item():\n",
        "        total_correct += 1\n",
        "      total_counter += 1\n",
        "\n",
        "    loss = loss_fcn(predictions, labels)\n",
        "    test_loss += loss.item() * data.size(0)\n",
        "\n",
        "  print(f\"Accuracy: {(total_correct/total_counter) * 100}\")\n",
        "  print(f\"Test Loss: {test_loss/len(test_loader)}\")"
      ],
      "metadata": {
        "id": "Ec1yAE_LyvzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_test_loss(model, test_loader)"
      ],
      "metadata": {
        "id": "eb3nz_eib3QW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6b572e7-bb83-426f-db88-62e1167d9f51"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 97.50999999999999\n",
            "Test Loss: 4.9732838382078395\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # Save the model:\n",
        "torch.save(model.state_dict(), 'trained_model.pth')"
      ],
      "metadata": {
        "id": "G9Qyb07zhk4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding Noise to the Test Dataset:\n",
        "\n",
        "One of the main purposes of this project is to see whether the Neural Network is robust enough to handle noise in a dataset. Given that the Neural Network is trained on a clean dataset, we want to see if adding noise from other distributions would make a huge difference in Neural Network classification accuracy.\n",
        "\n",
        "To add noise to the images, we use a Gaussian Mixture Model (GMM), and experiment with other models to add anomalous perturbations to the images. First we need to create a custom transformation:"
      ],
      "metadata": {
        "id": "QgzHgZ-ahWeC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AddRandomizedGaussianNoise(object):\n",
        "\n",
        "  # Adapted from this thread: https://discuss.pytorch.org/t/how-to-add-noise-to-mnist-dataset-when-using-pytorch/59745\n",
        "\n",
        "  def __init__(self, mean, st_dev):\n",
        "    self.mean = mean\n",
        "    self.st_dev = st_dev\n",
        "\n",
        "  def __call__(self, tensor):\n",
        "    return tensor + torch.randn(tensor.size()) * self.st_dev + self.mean\n",
        "\n",
        "  def __repr__(self):\n",
        "    return self.__class__.__name__ + f\"(Mean: {self.mean}, Standard Deviation: {self.st_dev})\""
      ],
      "metadata": {
        "id": "CSGeQj9Ehf4i"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Redefine the Testing Dataset to include randomized noise:\n",
        "\n",
        "test_dataset = datasets.MNIST(root='data/', train=False, download=True, transform=transforms.Compose([transforms.ToTensor(),\n",
        "                                                                                                      AddRandomizedGaussianNoise(random.randint(0, 5), 1)]))\n",
        "test_loader = DataLoader(test_dataset, batch_size=64)"
      ],
      "metadata": {
        "id": "aYgzmthqku6i"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_test_loss(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5N1NfrJmvKVm",
        "outputId": "67806993-4303-49a7-ddc7-25def02b93a0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 9.790000000000001\n",
            "Test Loss: 5579.413746050209\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "While running this, you should probably see that the test accuracy here (with noise) is much less than the test accuracy on similar images. This was to be expected especially considering we're adding extremely randomized noise to the test dataset."
      ],
      "metadata": {
        "id": "SXkEI8ZPwPzb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SMT Solvers:\n",
        "\n",
        "Satisfiable Modulo Theories (SMT) solvers are designed to assess whether logical formulas are satisfiable or not. In practical terms, it can be used to determine whether the design of a certain hardware/software is correct. Here we use SMT solvers to verify the \"correctness\" of the Neural Network."
      ],
      "metadata": {
        "id": "YkUYnK0bxVlD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PySMT"
      ],
      "metadata": {
        "id": "RuEHHFUqxWuk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}