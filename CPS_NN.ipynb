{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Neural Network Creation and Training:\n",
        "\n",
        "The following code creates a Neural Network from scratch, and trains the Neural Network based on the MNIST dataset"
      ],
      "metadata": {
        "id": "2amu9WKfX3RH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2lhZqdjXGio"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "gT2nHFLWXMBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Structure adopted partially from the PyTorch Documentation page, basic Neural Network:\n",
        "\n",
        "# Note: The input images from MNIST will be in 28x28 dimensions, so linear input should be in 784:\n",
        "\n",
        "class MNISTNeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "      super().__init__()\n",
        "      self.flatten = nn.Flatten()\n",
        "      self.linear_relu_stack = nn.Sequential(\n",
        "          nn.Linear(input_size, 256),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(256, 256),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(256, output_size),\n",
        "      )\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.flatten(x)\n",
        "      logits = self.linear_relu_stack(x)\n",
        "      return logits"
      ],
      "metadata": {
        "id": "uci5nrcPX0IL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the MNIST Data:\n",
        "\n",
        "In order to load the MNIST Data into the notebook, we will need to make use of datasets and DataLoader modules as provided in PyTorch:"
      ],
      "metadata": {
        "id": "SgBy5kHwahIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset = datasets.MNIST(root='data/', train=True, download=True, transform=transforms.ToTensor())\n",
        "training_loader = DataLoader(training_dataset, batch_size=64)\n",
        "\n",
        "test_dataset = datasets.MNIST(root='data/', train=False, download=True, transform=transforms.ToTensor())\n",
        "test_loader = DataLoader(test_dataset, batch_size=64)"
      ],
      "metadata": {
        "id": "sIR-SFAVYoew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Network:\n",
        "model = MNISTNeuralNetwork(input_size = 784, output_size=10).to(device)\n",
        "\n",
        "# Declare Loss Function and Optimizer:\n",
        "loss_fcn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "optimizer.zero_grad()"
      ],
      "metadata": {
        "id": "dlURr2GFadP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the Actual Network:\n",
        "\n",
        "For this project we will train the model on CLEAN data, but will test the neural network on noisy data. For this reason, we will need to alter the test dataset images later on in the notebook. However, for now - we will train the neural network that we have created over 100 epochs."
      ],
      "metadata": {
        "id": "aE4ffdMscq_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "past_training_loss = np.inf\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "  if epoch != 1:\n",
        "    past_training_loss = training_loss\n",
        "\n",
        "  training_loss = 0\n",
        "\n",
        "  for idx, (data, label) in enumerate(training_loader):\n",
        "    data = data.to(device)\n",
        "    labels = label.to(device)\n",
        "\n",
        "    data_flat = data.reshape(-1, 784)\n",
        "\n",
        "    # Make the prediction based on the NN model:\n",
        "    scores = model(data_flat)\n",
        "\n",
        "    # Compute the loss value based on the pre-defined loss function:\n",
        "    loss_val = loss_fcn(scores, labels)\n",
        "\n",
        "    # Backpropagation, allow for:\n",
        "    optimizer.zero_grad()\n",
        "    loss_val.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    training_loss += loss_val.item() * data.size(0)\n",
        "\n",
        "  print(f\"Epoch Number: {epoch}, Training Loss: {training_loss}\")\n",
        "  epoch += 1"
      ],
      "metadata": {
        "id": "2KJZ9hKxc54a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing Accuracy and Loss:\n",
        "\n",
        "Now we will test the model on the testing data, and assess the loss values and accuracy:"
      ],
      "metadata": {
        "id": "n9bOetPWbvpX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_correct = 0\n",
        "total_counter = 0\n",
        "\n",
        "test_loss = 0\n",
        "\n",
        "for data, labels in test_loader:\n",
        "\n",
        "  data = data.to(device)\n",
        "  labels = labels.to(device)\n",
        "\n",
        "  test_data_flat = data.reshape(-1, 784)\n",
        "\n",
        "  predictions = model(test_data_flat)\n",
        "\n",
        "  for pred, label in zip(predictions, labels):\n",
        "    if torch.argmax(pred).item() == label.item():\n",
        "      total_correct += 1\n",
        "    total_counter += 1\n",
        "\n",
        "  loss = loss_fcn(predictions, labels)\n",
        "  test_loss += loss.item() * data.size(0)\n",
        "\n",
        "print(f\"Accuracy: {(total_correct/total_counter) * 100}\")\n",
        "print(f\"Test Loss: {test_loss/len(test_loader)}\")"
      ],
      "metadata": {
        "id": "eb3nz_eib3QW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Save the model:\n",
        "torch.save(model.state_dict(), 'trained_model.pth')"
      ],
      "metadata": {
        "id": "G9Qyb07zhk4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding Noise to the Test Dataset:\n",
        "\n",
        "One of the main purposes of this project is to see whether the Neural Network is robust enough to handle noise in a dataset. Given that the Neural Network is trained on a clean dataset, we want to see if adding noise from other distributions would make a huge difference in Neural Network classification accuracy."
      ],
      "metadata": {
        "id": "QgzHgZ-ahWeC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CSGeQj9Ehf4i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}